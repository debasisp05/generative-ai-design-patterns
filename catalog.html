<table border="1">
    <caption>
        Catalog of Generative AI Design Patterns.
        See: https://github.com/lakshmanok/generative-ai-design-patterns/blob/main/catalog.html
    </caption>
    <tr>
        <th>Category</th>
        <td>Pattern Name</td>
        <td>Problem</td>
        <td>Best Practice Solution</td>
        <td>Reference</td>
        <td>Tradeoffs and Alternatives</td>
        <td>Example demonstrating usage</td>
    </tr>
    <tr>
        <th>Controlling Style</th>
        <td>Grammar</td>
        <td>LLM needs to generate text that follows a strict grammar. 
            This could be as simple as JSON, XML, etc. or as complex as a chess game notation.</td>
        <td>Express constraints in Backus-Naur Form (BNF), and augment demonstration examples with the grammar.</td>
        <td>
            <ul>
                <li><a href="https://arxiv.org/abs/2305.19234">Grammar Prompting for Domain-Specific Language Generation with LLMs</a></li>
                <li><a href="https://github.com/ggerganov/llama.cpp/blob/master/grammars/README.md">GNBF Guide in llama.cpp</a></li>
            </ul>

        </td>
        <td>Providing just examples is inconsistent, and fails more frequently. <br/>
            For simple tasks, possible to use a template approach</td>
        <td>
            <ul>
            <li><a href="https://til.simonwillison.net/llms/llama-cpp-python-grammars">Llama CPP Python Grammars</a></li>
            <li><a href="https://github.com/ejones/llama-journey">A game with Llama CPP Grammar</a></li>
            </ul>
        </td>
    </tr>
    <tr>
        <th></th>
        <td>Self Check</td>
        <td>Get an LLM to say &quot;I don&#39;t know&quot; rather than hallucinating facts or details.</td>
        <td>Get an LLM to generate multiple answers in parallel. 
            Do a self-consistency check, and if they diverge, it&#39;s probably hallucinating.</td>
        <td><a href="https://arxiv.org/abs/2303.08896">SelfCheckGPT: Zero-Resource Black-Box Hallucination Detection</a>
        </td>
        <td>Fine-tuning the LLM or using examples for which &quot;I don&#39;t know&quot; 
            is the right answer is less reliable.</td>
        <td>Use output rails in <a href="https://github.com/NVIDIA/NeMo-Guardrails">NeMo-Guardrails</a>
        </td>
    </tr>
    <tr>
        <th></th>
        <td>Reverse Neutralization</td>
        <td>LLM needs to match a specific tone and voice, such as your company style or personal style.</td>
        <td>Take content created in desired style, ask LLM to rephrase in neutral language at 10th grade level. 
            Reverse the inputs and outputs and use it to Adapter Tune a model that will take LLM outputs
            and rephrase them in your style.</td>
        <td>
            <ul>
            <li><a href="https://community.openai.com/t/train-a-gpt-model-in-my-tone/119175/31"> Train a GPT Model in My Tone </a></li>
            <li><a href="https://arxiv.org/abs/2308.07968">Teach LLMs to Personalize</a></li>
            <li><a href="https://technicalwriting.tools/posts/style-guide-fine-tuning/">Fine-tuning an LLM into a style guide editor</a></li>
            </ul>

        </td>
        <td>
            <ul>
                <li>It&#39;s hard to create a dataset that covers all possible topics, so using only a fine-tuned LLM tends to lose of creativity.</li>
                <li>If all you need is a style checker, you can use an off-the-shelf LLM to enforce a style guide. </li>
                <li>If your content does cover all possible topics, you can create a RAG and fine-tune every stage of it (see: https://arxiv.org/abs/2308.07968) </li>
                <li>If the matching is in terms of instructions, use the Style Transfer pattern.</li>
            </ul>
        </td>
        <td></td>
    </tr>
    <tr>
        <th></th>
        <td>Style Transfer</td>
        <td>Get an LLM to follow instructions such as &quot;more positive&quot; but get the right level of positivity.</td>
        <td>Use augmented zero-shot prompting, which augments what&#39;s desired with variations that are non-exemplars.</td>
        <td><a href="https://aclanthology.org/2022.acl-short.94.pdf">A Recipe For Arbitrary Text Style Transfer with Large Language Models</a></td>
        <td>See Reverse Neutralization</td>
        <td></td>
    </tr>
    <tr>
        <th>Adding Capability</th>
        <td>Adapter Tuning</td>
        <td>LLM has to do a task that the foundational model was not trained to do.</td>
        <td>Create instruction dataset of input-output pairs that demonstrate task; use LoRA</td>
        <td><a href="https://arxiv.org/abs/2106.09685">Low Rank adapation of LLMs</a></td>
        <td>
            <ul>
                <li>For tasks that are similar to the foundational model, consider Few Shot.</li>
                <li>More efficient than fine-tuning of all the weights (instruction tuning).</li>
            </ul></u>
        </td>
        <td></td>
    </tr>
    <tr>
        <th></th>
        <td>Few Shot</td>
        <td>LLM has to do a task that the foundational model was not trained to do, but lies within its frontier capability.</td>
        <td>Add a few examples of task input and output to the prompt.</td>
        <td><a href="https://arxiv.org/abs/2005.14165">Language Models are Few-Shot Learners</a></td>
        <td>If beyond frontier capability, may need to do Adapter Tuning.</td>
        <td></td>
    </tr>
    <tr>
        <th></th>
        <td>Chain of Thought</td>
        <td>Teach the LLM the steps it has to follow to get to an answer.</td>
        <td></td>
        <td></td>
        <td>
            <ul>
                <li>Limited to linear flows only. For complex flows, consider Tree of Thought.</li>
                <li>In many cases, the LLM can self-reflect and get the list of steps to follow.</li>
            </ul>
        </td>
        <td></td>
    </tr>
    <tr>
        <th></th>
        <td>Tree of Thought</td>
        <td>Teach the LLM the steps it has to follow to get to an answer.</td>
        <td></td>
        <td></td>
        <td>For simple linear flows, can use Chain of Thought</td>
        <td></td>
    </tr>
    <tr>
        <th></th>
        <td>Evol Instruct</td>
        <td>Teach the LLM new capabilities efficiently</td>
        <td>Organize the training data starting from tutorials and then evolve the examples to increase complexity.</td>
        <td>
            <ul>
                <li><a href="https://arxiv.org/abs/2306.11644">Textbooks are all you need</a></li>
                <li><a href="https://arxiv.org/abs/2304.12244">WizardLM: Empowering LLMs to follow complex instructions</a></li>
            </ul>
        </td>
        <td>
            <ul>
                <li>Applies to all the other patterns in this category.</li>
                <li>Could also increase model complexity along with instruction complexity, but more common to use Adapter Tuning where complexity is fixed. </li>
            </ul>
        </td>
        <td></td>
    </tr>
    <tr>
        <th>Architectural</th>
        <td>Agents</td>
        <td>Orchestrate a complex workflow consisting of information retrieval, function calling, and conversation maintenance.</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <th></th>
        <td>RAG</td>
        <td>Use knowledge the LLM was not trained on, such as confidential information or new information in generation.</td>
        <td></td>
        <td></td>
        <td>Considerations include appropriate chunking and reranking strategies.</td>
        <td></td>
    </tr>
    <tr>
        <th></th>
        <td>Tool Functions</td>
        <td>Incorporate non-language tasks whose results are needed for generation</td>
        <td>Teach LLM to emit text of function name + parameters to call backend functions </td>
        <td></td>
        <td>See Grammar for how to ensure compliance to API specs.  See also Domain Languages.</td>
        <td></td>
    </tr>
    <tr>
        <th></th>
        <td>Domain Languages</td>
        <td>Incorporate non-language tasks whose results are needed for generation</td>
        <td>Teach LLM to emit a domain specific language (SQL, Matplotlib, etc.) that will be processed by a sandbox or backend service.</td>
        <td></td>
        <td>See Grammar for how to ensure compliance to API specs.  See also Tool Functions.</td>
        <td></td>
    </tr>    
    <tr>
        <th></th>
        <td>Automatic Prompt Tuning</td>
        <td>Ensure that as model versions change, your prompts continue to perform well. </td>
        <td>Evaluate multiple candidate prompts against desired outputs as part of both development and deployment. Incorporate into CI/CD ML pipelines.</td>
        <td></td>
        <td>Can also be used to counteract drift.</td>
        <td></td>
    </tr>
    <tr>
        <th>Controlling Context</th>
        <td>Example Selection</td>
        <td>Guard against information leakage.</td>
        <td>Restrict world knowledge.</td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <th></th>
        <td>Conversation State</td>
        <td>How can the LLM remember previous turns of the conversation?</td>
        <td>Summarize the previous conversation and add it to context.</td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <th></th>
        <td>Guardrails</td>
        <td>Guard against toxicity, bias, information leakage, etc.</td>
        <td>Pre- and post- processing of LLM inputs and outputs to ensure compliance to goals</td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <th>Meeting Constraints</th>
        <td>Distillation</td>
        <td>Reduce cost of inference within acceptable quality bounds.</td>
        <td>Teach student model on output of larger model.</td>
        <td></td>
        <td>See also Quantization</td>
        <td></td>
    </tr>
    <tr>
        <th></th>
        <td>History Cache</td>
        <td>Ensure same answer to repeat query.</td>
        <td>Cache previous queries and responses to provide deterministic behavior</td>
        <td></td>
        <td>Should you use semantic search or exact match to see if this is a repeat query?</td>
        <td></td>
    </tr>
    <tr>
        <th></th>
        <td>Context Window</td>
        <td>How to fit examples, conversation state, etc. into limited context window</td>
        <td></td>
        <td></td>
        <td></td>
        <td></td>
    </tr>
    <tr>
        <th></th>
        <td>Quantization</td>
        <td>Reduce cost of inference within acceptable quality bounds.</td>
        <td></td>
        <td></td>
        <td>Would use collection of techniques (quantization, distillation, caching, etc.)</td>
        <td></td>
    </tr>
</table>
